{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iris Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishan53/Iris-classification/blob/master/Iris_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMJtOdlg8qxE"
      },
      "source": [
        "# Iris Classification\n",
        "\n",
        "In this example, we will develop a couple of machine learning models to classify different species of Iris, specifically iris Setosa, Versicolor, Virginica. We will solve the problem one step at a time to understand the process of machine learning and the some of the math behind it. Don't worry if you are not great at math, it's not as complicated as you might think.\n",
        "\n",
        "The [iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) is available as part of [Sci-kit learn](https://scikit-learn.org/stable/), so it's a nice place to start and demonstrates the basic idea.\n",
        "\n",
        "\n",
        "\n",
        "The data set contains 50 samples from each of three species of Iris. Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. These four features can be used to train a machine learning model to classify or predict the species of a particular Iris."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iFSzmzVs8qxF",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R_ym5ascgYR9"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P6y8QRu88qxI",
        "colab": {}
      },
      "source": [
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rr_zxf6x8qxJ",
        "colab": {}
      },
      "source": [
        "print(iris.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tOHFyZc88qxM",
        "colab": {}
      },
      "source": [
        "iris.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-qfS1nEd8qxO",
        "colab": {}
      },
      "source": [
        "len(iris.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jZn0FLlfArFH",
        "colab": {}
      },
      "source": [
        "iris.data[0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EYFImHIcBPca",
        "colab": {}
      },
      "source": [
        "iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y7GbVdO08qxQ"
      },
      "source": [
        "## Plot Sepal Width and Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mq0K04SH8qxQ",
        "colab": {}
      },
      "source": [
        "# Select all rows and only first two columns (sepal length/width)\n",
        "X = iris.data[:, :2]\n",
        "\n",
        "# Target will be used to plot samples in different colors for different species\n",
        "Y = iris.target\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=Y)\n",
        "plt.xlabel('Sepal Length (cm)')\n",
        "plt.ylabel('Sepal Width (cm)')\n",
        "plt.title('Sepal size distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CBPVmV3YFVzI",
        "colab": {}
      },
      "source": [
        "# Select all rows and only last two columns (sepal length/width)\n",
        "X = iris.data[:, 2:]\n",
        "\n",
        "# Target will be used to plot samples in different colors for different species\n",
        "Y = iris.target\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=Y)\n",
        "plt.xlabel('Petal Length')\n",
        "plt.ylabel('Petal Width')\n",
        "plt.title('Petal size distribution (cm)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZBCYsK88qxS"
      },
      "source": [
        "## Logistic Regression Model\n",
        "\n",
        "We will create a basic model that predicts if a sample is one of the three species or not.\n",
        "\n",
        "The structure of the model appears below:\n",
        "\n",
        "\n",
        "\n",
        "In 1958, an image recognition machine was developed with 400 [perceptrons](https://en.wikipedia.org/wiki/Perceptron) at the Cornell Aeronautical Lab. It showed some inital promise, but turned out to have limited capability. It paved the way for more sophisticated [multi-layer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron) which are more powerful and widely used in deep learning models today. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pkp1H48t-jZ",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid Function\n",
        "\n",
        "The [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is commonly used in logistic regression and artificial neurons. It's one way to convert continous values into more of a binary value. This also called an activation function in neural networks. There are many other types of activation functions in use today."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "So99N93VJ9Vy",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1.0/(1 + math.e ** (-z))\n",
        "\n",
        "x = [i * 0.1 for i in range(-50, 51)]\n",
        "y = [sigmoid(z) for z in x]\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Logistic Sigmoid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SPj_r-Zt-jb",
        "colab_type": "text"
      },
      "source": [
        "## Prediction Function\n",
        "\n",
        "This function takes one sample, multiplies by the weights, adds a bias, and passes the sum through a Sigmoid function. This function will be used to determine weights and biases during training as well as make predictions after training is complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QaERzVGXIoTc",
        "colab": {}
      },
      "source": [
        "def predict(sample):\n",
        "  result  = 0.0\n",
        "  for i in range(len(sample)):\n",
        "    result = result + weights[i] * sample[i]\n",
        "    \n",
        "  result = result + bias\n",
        "  return sigmoid(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNAtSCEPt-jd",
        "colab_type": "text"
      },
      "source": [
        "## Loss and Cost Functions\n",
        "\n",
        "The loss function compares the predicted output with the expected output for a single training sample. If the absolute difference between the predicted and expected output is high, the loss should be high. The loss function here is a bit more complicated than a simple difference between predicted output and expected output, because it is known to make training easier.\n",
        "\n",
        "$$\\mathcal L(y, \\hat y) = -(y \\log \\hat y + (1-y) \\log (1 - \\hat y))$$\n",
        "\n",
        "Where $\\mathcal L$ is the loss, $y$ is the training sample or ground truth, and $\\hat y$ is the predicted value from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bhN98ZwJNcnL",
        "colab": {}
      },
      "source": [
        "def loss(y_train, y_predicted):\n",
        "  return -(y_train * math.log(y_predicted) + (1.0 - y_train) * math.log(1 - y_predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru0QNMpUt-jg",
        "colab_type": "text"
      },
      "source": [
        "We can see how this function behaves for a fixed training value and a range of predicted values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89HtEbUpt-jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = 0.9\n",
        "x = [i * 0.1 for i in range(1, 9)]\n",
        "y = [loss(y_train, yp) for yp in x]\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Loss near %0.2f' % y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9w9xQEkt-ji",
        "colab_type": "text"
      },
      "source": [
        "The cost function is the average of the loss over all training samples:\n",
        "\n",
        "$$\\mathcal J = \\frac{1}{m} \\sum_{i=0}^{m} \\mathcal L(y, \\hat y)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSLUEK-pt-jj",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Gradient descent attempts to minimize the cost function by iteratively adjusting weights and biases in the direction of lower cost. This achieved by computing the derivative (gradient) or slope of the cost function for each of the weights and biases.\n",
        "\n",
        "The general equations are\n",
        "\n",
        "$$w_{k+1} = w_{k} - \\alpha \\frac{\\partial \\mathcal J}{\\partial w}$$\n",
        "\n",
        "$$b_{k+1} = b_{k} - \\alpha \\frac{\\partial \\mathcal J}{\\partial b}$$\n",
        "\n",
        "Where $w_{k}$ and $b_{k}$ are the current values of weights and biases, and $w_{k+}$ and $b_{k+1}$ are the next values after one iteration. The number $\\alpha$ is the learning rate, which is a tuning parameter or hyperparameter. If the learning rate is too small, it will take longer to train, whereas too large of a training value might result in wild oscillations that never converge on an optimum solution.\n",
        "\n",
        "To picture how this works, let's consider an example in two dimensions:\n",
        "\n",
        "$$y = x^2 + \\frac{x}{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-bXBig8t-jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parabola(x):\n",
        "    return x**2 + x/2.0\n",
        "\n",
        "x = [i * 0.1 for i in range(-10, 11)]\n",
        "y = [parabola(xi) for xi in x]\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Simple function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaQJuoXzt-jl",
        "colab_type": "text"
      },
      "source": [
        "The first derivative, or the slope at any point $x$ is:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x} = 2x + 0.5$$\n",
        "\n",
        "To find the minimum, iterate using this equation:\n",
        "\n",
        "$$x_{k+1} = x_{k} - \\alpha \\frac{\\partial y}{\\partial x}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDK0JIfst-jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_k = 0.0\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def derivative(x):\n",
        "  return 2*x + 0.5\n",
        "\n",
        "for i in range(5):\n",
        "    gradient = derivative(x_k)\n",
        "    x_k = x_k - learning_rate*gradient\n",
        "\n",
        "print('Estimated minimum %0.2f, %0.2f' % (x_k, parabola(x_k)))\n",
        "print('Derivative (gradient) %0.2f' % gradient)\n",
        "\n",
        "x = [i * 0.1 for i in range(-10, 11)]\n",
        "y = [parabola(xi) for xi in x]\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.plot(x_k, parabola(x_k), 'ro')\n",
        "line_x = [x_k - 0.5, x_k + 0.5]\n",
        "line_y = [gradient*(xi-x_k)+parabola(x_k) for xi in line_x]\n",
        "plt.plot(line_x, line_y)\n",
        "plt.title('Simple function')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmwF2NyYt-jp",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation Algorithm\n",
        "\n",
        "[Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) is a widely used algorithm for training machine learning models. It attempts to learn weights and biases by iteratively adjusting these values in order to minimize the loss between predicted and expected output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90LurR8iP_z1",
        "colab": {}
      },
      "source": [
        "num_features = iris.data.shape[1]\n",
        "\n",
        "def train_one_epoch(x_train_samples, y_train_samples):\n",
        "  cost = 0.0\n",
        "  dw = [0.0] * num_features\n",
        "  db = 0.0\n",
        "\n",
        "  global bias, weights\n",
        "\n",
        "  m = len(x_train_samples)\n",
        "  for i in range(m):\n",
        "    x_sample = x_train_samples[i]\n",
        "    y_sample = y_train_samples[i]\n",
        "    predicted = predict(x_sample)\n",
        "    cost = cost + loss(y_sample, predicted)\n",
        "    \n",
        "    # dz is the derivative of the loss function\n",
        "    dz = predicted - y_sample\n",
        "    \n",
        "    for j in range(len(weights)):\n",
        "      dw[j] = dw[j] + x_sample[j] * dz\n",
        "    db = db + dz\n",
        "  \n",
        "  cost = cost / m\n",
        "  db = db / m\n",
        "  bias = bias - learning_rate*db\n",
        "  for j in range(len(weights)):\n",
        "    dw[j] = dw[j] / m\n",
        "    weights[j] = weights[j] - learning_rate*dw[j]\n",
        "  \n",
        "  return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPtAVgMvt-jr",
        "colab_type": "text"
      },
      "source": [
        "## Training Algorithm\n",
        "\n",
        "This algorithm with iterate through the training data many times and call the backpropagation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zUeIwAvRlrYn",
        "colab": {}
      },
      "source": [
        "# Model will \"learn\" values for the weights and biases\n",
        "\n",
        "weights = [0.0] * num_features\n",
        "bias = 0.0\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "epochs = 2000\n",
        "\n",
        "x_train_samples = iris.data\n",
        "y_train_samples = [1 if y == 2 else 0 for y in iris.target]\n",
        "\n",
        "loss_array = []\n",
        "for epoch in range(epochs):\n",
        "  loss_value = train_one_epoch(x_train_samples, y_train_samples)\n",
        "  loss_array.append(loss_value)\n",
        "\n",
        "plt.plot(range(epochs), loss_array)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.title('Loss vs. Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dXTIJSxt-jv",
        "colab_type": "text"
      },
      "source": [
        "## Make Predictions\n",
        "\n",
        "Once the model is trained, it can be used to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0YzlLmmumyD9",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "m = len(x_train_samples)\n",
        "correct = 0\n",
        "for i in range(m):\n",
        "  sample = x_train_samples[i]\n",
        "  value = predict(sample)\n",
        "  predictions.append(value)\n",
        "  if value >= 0.5:\n",
        "    value = 1\n",
        "  else:\n",
        "    value = 0\n",
        "  if value == y_train_samples[i]:\n",
        "    correct = correct + 1.0\n",
        "\n",
        "plt.plot(range(m), predictions, label='Predicted')\n",
        "plt.plot(range(m), y_train_samples, label='Ground truth')\n",
        "plt.ylabel('Prediction')\n",
        "plt.xlabel('Sample')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy: %.2f %%' % (100 * correct/m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH4RMMVGt-jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}